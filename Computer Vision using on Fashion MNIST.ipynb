{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion MNIST is a MNIST-like fashion product database consists 60.000 training images and 10.000 test images. Each image is a greyscale and associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges in Computer Vision (CV)\n",
    "\n",
    "\n",
    "One of the most useful in Deep Learning for Computer Vision are Convolution Neural Networks (CNNs). Today they are the most sophisticated fancy computer vision application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the csv file into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "df_test = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_test: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['label'], axis=1)\n",
    "y_train = df_train['label']\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a train, val and test split for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/6, random_state=0)\n",
    "\n",
    "X_test = df_test.drop(['label'], axis = 1)\n",
    "y_test = df_test['label']\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'y_val shape: {y_val.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    X : pandas.DataFrame (images)\n",
    "    y : pandas.Series (labels)\n",
    "    transform : list of data augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y=None, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # PyTorch automatically divide image data by 255 when its data type is np.uint8 \n",
    "        # (np.uint8 : unchanged sign value [0 ~ 255])\n",
    "        image = self.X.iloc[index, :].values.astype(np.uint8).reshape((28, 28, 1)) # (height, width, channels)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.y is not None: # training\n",
    "            return image, self.y.iloc[index]\n",
    "        else: # prediction\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "lr = 0.01\n",
    "NUM_CLASS = 10\n",
    "TRAIN_EPOCH = 20\n",
    "IMAGE_SIZE = 28\n",
    "CHANNEL = 1\n",
    "\n",
    "#Class names\n",
    "class_clothing = {0: 'T-shirt/top',\n",
    "                  1: 'Trouser',\n",
    "                  2: 'Pullover',\n",
    "                  3: 'Dress',\n",
    "                  4: 'Coat',\n",
    "                  5: 'Sandal',\n",
    "                  6: 'Shirt',\n",
    "                  7: 'Sneaker',\n",
    "                  8: 'Bag',\n",
    "                  9: 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "])\n",
    "\n",
    "train_dataset = FashionMNIST(X_train, y_train, transform=My_transform)\n",
    "\n",
    "val_dataset = FashionMNIST(X_val, y_val, transform=My_transform)\n",
    "\n",
    "test_dataset = FashionMNIST(X_test, y_test, transform=My_transform)\n",
    "\n",
    "\n",
    "Train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              shuffle=False\n",
    "                              )\n",
    "Val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                               batch_size = BATCH_SIZE,\n",
    "                               shuffle=False)\n",
    "\n",
    "Test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              shuffle=False\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lab = train_dataset.__getitem__(0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_img = img.numpy().flatten().reshape((28,28))\n",
    "\n",
    "plt.imshow(full_img, cmap='gray')\n",
    "plt.title(f'label is {lab}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(Train_dataloader)\n",
    "images, labels = next(train_iter)\n",
    "\n",
    "print(f'train_dataset_img shape: torch.Size([BATCH_SIZE, CHANNEL, IMAGE_SIZE, IMAGE_SIZE])')\n",
    "print(f'train_dataset_lab shape: torch.Size([BATCH_SIZE])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(images, nrow=8)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.imshow(grid.numpy().transpose((1,2,0)))\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(Test_dataloader)\n",
    "images, labels = next(test_iter)\n",
    "\n",
    "grid = make_grid(images, nrow=8)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idee: Wir interpretieren die Abfolge der Bild-Spalten als Sequenz und die Eintr√§ge der Bilderspalten als Feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into convolutional neural networks, let's take a quick overview of the traditional techniques used in computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Traditional Approaches\n",
    "\n",
    "Various techniques work well for simpler problems, but if the task complex, they are no substitute for deep CNNS. Let's discuss two simple approaches:\n",
    "\n",
    "1. KNN (K-Nearest Neighbours):\n",
    "\n",
    "Each image is matched with all images in data. The top K with minimum distances (e.g. L1 distance or L2 distance) are selected. The majority class of those top K is predicted as output class of the image.\n",
    "\n",
    "Problem: If we have the same object in a picture with different illumination, orientation or location then KNN would give highly non-zero distance for the same images.\n",
    "\n",
    "2. Linear Classifiers:\n",
    "\n",
    "They use a parametric approach where each pixel value is considered as a parameter.\n",
    "It is a weighted sum of the pixel values with the dimension of the weights matrix depending on the number of outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Neural Networks\n",
    "\n",
    "We recall the basics of neural networks here und discuss some properties. \n",
    "\n",
    "There are various activation functions which can be used. Let's discuss some of the popular options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activiation Functions\n",
    "\n",
    "#### Sigmoid activation\n",
    "\n",
    "- Equation: sigma(x) = 1/(1 + e ^ (-x) ) with input space from (- infinity, + infinity) to (0,1)\n",
    "\n",
    "- It has some problems and it is almost never used in CNNs:\n",
    "    1. If the input beyond -5 or 5, the output will be very close to 0 and 1 respectively. That means, the gradients are almost zero. As we know that gradients get multiplied in backpropagation, so this small grdient will stop backpropagation into further layers.\n",
    "\n",
    "- Outputs are nont zero-centered\n",
    "    2. As we know the outputs are between 0 and 1, i.e. all the gradients of the next layer will be either positive or negative. So the path to optimum will be zig-zag. \n",
    "    \n",
    "- Taking the exponential function is computationally expensive\n",
    "    3. It has a slight negative impact\n",
    "    \n",
    "#### tanh activation\n",
    "\n",
    "- Equation: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "\n",
    "- It is always preferred over sigmoid because the outputs are in range(-1,1). But it will still result in killigin the gradient.\n",
    "\n",
    "#### ReLu (Rectrified LInear Unit) activation\n",
    "\n",
    "- Equation: f(x) = max(0,x)\n",
    "\n",
    "- It is the most commonly used activation function for CNNs. It has some advantages:\n",
    "    - Gradient will not saturate in the positive region\n",
    "    - Computationally very efficient as simple thresholding is required\n",
    "    - Empirically found to converge faster than sigmoid or tanh\n",
    "    \n",
    "- But it has the following disadvantages:\n",
    "    - Output is not zero-centered and always positive\n",
    "    - Gradient is killed for x < 0.\n",
    "    \n",
    "Summary: ReLU is mostly the activation function of choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Convolution Neural Networks\n",
    "\n",
    "### A CNN typically consists of 3 types of layers:\n",
    "\n",
    "1. Convolution Layer\n",
    "2. Pooling Layer\n",
    "3. Fully connected Layer\n",
    "\n",
    "### Convolution Layer\n",
    "\n",
    "Since Convolution layers from the crux of the network. Each layer can be visualized in the form of a block. For instance in the case of FASHION-MNIST, input layer would have the following form: (1, 28, 28)\n",
    "\n",
    "The original image has 28x28 in height and width. Sometimes the depth is 3 which corresponds to the Red, Green and Blue colors, which form the basis of colored images.\n",
    "\n",
    "A Convolultion layer is formed by running a filter over it. A filter is another block of smaller height and width but same depth which is swept over this base block.\n",
    "\n",
    "We start this filter from the top left corner and sweep it till the bottom it till the bottom left corner. This filter is a set of weights, i.e. in our case 5x5 = 25 + 1 weights. Our term stride, which is the number of cells to move in each step is one (stride=1). Then the output size will be: (Convolutional layer dim - Filter dim) / Stride + 1. The output is 24 pixels which we get from this formula. This is undesirable in case of deep neural networs where the size would become very small too early. To prevent this, we generally use a stride of 1 along with zero-padding of size (Filter dim - 1)/2. Zero-padding is nothing but additional zero-value pixels towards the border of the images. The required padding is (5-1)/2 = 2. Now the image-size remains the same. \n",
    "\n",
    "#### Pooling Layer\n",
    "\n",
    "Pooly layers are used to reduce the size of imag. They work by sampling in each layer using filters. If we use 2x2 filter with stride 1 and max-pooling, we get the following response:\n",
    "(28 - 14 + 2) = 16\n",
    "\n",
    "Generally, max-pooling is used but other options like average pooling ca be considered too.\n",
    "\n",
    "#### Fully Connectecd Layer\n",
    "\n",
    "At the end of convolution and pooling layers, networks generally use fully-connected layers in which each pixel is considered as a seperate neuron just like a regular neural network. The last fully-connect layer will contain as many neurons as the number of classes to be predicted. In our case 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_of_class):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7 * 7 * 32, num_of_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)  # convert data shape (64, 1, 28, 28) -> (64, 1*28*28) = [64,784]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_of_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28*28*1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, num_of_classes) # 10 because we have 10 different classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # convert data shape (64, 1, 28, 28) -> (64, 1*28*28) = [64,784]\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    mean_train_loss = []\n",
    "    mean_val_loss = []\n",
    "    \n",
    "    for epoch in range(TRAIN_EPOCH):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for batch_id, (images, labels) in enumerate(Train_dataloader):\n",
    "            \n",
    "            # zero the paramters gradient\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # feedforward -> backward -> optimize\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            train_loss.append(loss.data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch_id * BATCH_SIZE) % (BATCH_SIZE * 100) == 0:\n",
    "                print(f'{batch_id * BATCH_SIZE} / 50000')\n",
    "        \n",
    "        # Each Epoch we validate our model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_id, (images, labels) in enumerate(Val_dataloader):\n",
    "                \n",
    "                # feedforward -> loss\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss.append(loss.data)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        mean_train_loss.append(np.mean(train_loss))\n",
    "        mean_val_loss.append(np.mean(val_loss))\n",
    "    \n",
    "        print('Epoch:[{}/{}], train loss : {:.4f}, valid loss : {:.4f}, val acc : {:.2f}%'\\\n",
    "              .format(epoch+1, TRAIN_EPOCH, np.mean(train_loss),\\\n",
    "                               np.mean(val_loss), 100*correct/total))\n",
    "    \n",
    "    # Plotting our test and val losses\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.plot(mean_train_loss, label='train')\n",
    "    ax.plot(mean_val_loss, label='val')\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(lines, labels, loc='best')\n",
    "    plt.ylim(0.05,0.8)\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_id, (images, labels) in enumerate(Test_dataloader):\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(NUM_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_val(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = train_val(model)\n",
    "    test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
